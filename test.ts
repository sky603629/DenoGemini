#!/usr/bin/env -S deno run --allow-net --allow-env --allow-read

// ç®€å•çš„æµ‹è¯•è„šæœ¬æ¥éªŒè¯æœåŠ¡å™¨åŠŸèƒ½
import { configManager } from "./config/env.ts";

async function testServer() {
  try {
    console.log("ğŸ§ª æµ‹è¯•Geminiåˆ°OpenAI APIæœåŠ¡å™¨...\n");

    // æµ‹è¯•1ï¼šåŠ è½½é…ç½®
    console.log("1. æµ‹è¯•é…ç½®åŠ è½½...");
    try {
      await configManager.loadConfig();
      console.log("âœ… é…ç½®åŠ è½½æˆåŠŸ");
      console.log(`   - APIå¯†é’¥æ•°é‡: ${configManager.getApiKeyCount()}`);
      console.log(`   - ç«¯å£: ${configManager.getConfig().port}`);
    } catch (error) {
      console.log("âŒ é…ç½®åŠ è½½å¤±è´¥:", (error as Error).message);
      console.log("   è¯·ç¡®ä¿æ‚¨å·²åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®äº†GEMINI_API_KEYS");
      return;
    }

    // æµ‹è¯•2ï¼šæµ‹è¯•æ¨¡å‹æœåŠ¡
    console.log("\n2. æµ‹è¯•æ¨¡å‹æœåŠ¡...");
    try {
      const { modelService } = await import("./services/modelService.ts");
      const models = await modelService.getAvailableModels();
      console.log(`âœ… å·²è·å– ${models.length} ä¸ªå¯ç”¨æ¨¡å‹`);
      if (models.length > 0) {
        console.log(`   - ç¤ºä¾‹æ¨¡å‹: ${models[0].name}`);
      }
    } catch (error) {
      console.log("âŒ æ¨¡å‹æœåŠ¡æµ‹è¯•å¤±è´¥:", (error as Error).message);
    }

    // æµ‹è¯•3ï¼šæµ‹è¯•åŸºæœ¬è½¬æ¢
    console.log("\n3. æµ‹è¯•è¯·æ±‚è½¬æ¢...");
    try {
      const { transformOpenAIRequestToGemini } = await import("./transformers/openaiToGemini.ts");
      const openaiRequest = {
        model: "gemini-1.5-pro",
        messages: [
          { role: "user" as const, content: "ä½ å¥½ï¼Œä¸–ç•Œï¼" }
        ]
      };

      const geminiRequest = await transformOpenAIRequestToGemini(openaiRequest, "gemini-1.5-pro");
      console.log("âœ… è¯·æ±‚è½¬æ¢æˆåŠŸ");
      console.log(`   - å†…å®¹é¡¹: ${geminiRequest.contents.length} ä¸ª`);
    } catch (error) {
      console.log("âŒ è¯·æ±‚è½¬æ¢å¤±è´¥:", (error as Error).message);
    }

    // æµ‹è¯•4ï¼šæµ‹è¯•å“åº”è½¬æ¢
    console.log("\n4. æµ‹è¯•å“åº”è½¬æ¢...");
    try {
      const { transformGeminiResponseToOpenAI } = await import("./transformers/geminiToOpenAI.ts");
      const geminiResponse = {
        candidates: [{
          content: {
            role: "model" as const,
            parts: [{ text: "ä½ å¥½ï¼æˆ‘ä»Šå¤©èƒ½ä¸ºæ‚¨åšäº›ä»€ä¹ˆï¼Ÿ" }]
          },
          finishReason: "STOP" as const
        }],
        usageMetadata: {
          promptTokenCount: 10,
          candidatesTokenCount: 15,
          totalTokenCount: 25
        }
      };

      const openaiRequest = {
        model: "gemini-1.5-pro",
        messages: [{ role: "user" as const, content: "ä½ å¥½" }]
      };

      const openaiResponse = transformGeminiResponseToOpenAI(geminiResponse, openaiRequest, "test-123");
      console.log("âœ… å“åº”è½¬æ¢æˆåŠŸ");
      console.log(`   - é€‰æ‹©é¡¹: ${openaiResponse.choices.length}`);
      console.log(`   - ä½¿ç”¨é‡: ${openaiResponse.usage?.total_tokens} ä¸ªtoken`);
    } catch (error) {
      console.log("âŒ å“åº”è½¬æ¢å¤±è´¥:", (error as Error).message);
    }

    console.log("\nğŸ‰ åŸºæœ¬æµ‹è¯•å®Œæˆï¼");
    console.log("\nè¦å¯åŠ¨æœåŠ¡å™¨ï¼Œè¯·è¿è¡Œ:");
    console.log("  deno task dev");
    console.log("\nç„¶åæµ‹è¯•:");
    console.log("  curl http://localhost:8000/v1/models");

  } catch (error) {
    console.error("âŒ æµ‹è¯•å¤±è´¥:", error);
  }
}

if (import.meta.main) {
  testServer();
}
